{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV3 model initiation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "\n",
    "class MobileNetV3Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MobileNetV3Classifier, self).__init__()\n",
    "        self.mobilenet = mobilenet_v3_small(pretrained=True)\n",
    "        \n",
    "        in_features = self.mobilenet.classifier[3].in_features\n",
    "        self.mobilenet.classifier[3] = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mobilenet(x)\n",
    "\n",
    "num_classes = 104\n",
    "model = MobileNetV3Classifier(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.jpg: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1.jpg: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2.jpg: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Training dataset - Encoding\n",
    "\n",
    "import csv\n",
    "\n",
    "def normalize_label(label):\n",
    "    return label.strip().lower()\n",
    "\n",
    "class_name_to_id = {}\n",
    "with open(r\"C:\\Users\\Hithesh\\Downloads\\Kaush Stuff\\FoodSeg103 Stuff\\category_id.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            idx, name = parts\n",
    "            class_name_to_id[normalize_label(name)] = int(idx)\n",
    "\n",
    "num_classes = max(class_name_to_id.values()) + 1\n",
    "\n",
    "train_labels_encoded = {}\n",
    "with open(r\"C:\\Users\\Hithesh\\Downloads\\Kaush Stuff\\FoodSeg103_export\\train\\labels.csv\", newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        filename = row['filename']\n",
    "        labels = row['labels'].split(',')\n",
    "        label_indices = []\n",
    "        for label in labels:\n",
    "            norm_label = normalize_label(label)\n",
    "            if norm_label in class_name_to_id:\n",
    "                idx = class_name_to_id[norm_label]\n",
    "                if idx < num_classes:\n",
    "                    label_indices.append(idx)\n",
    "        label_vector = [0] * num_classes\n",
    "        for idx in label_indices:\n",
    "            label_vector[idx] = 1\n",
    "        train_labels_encoded[filename] = label_vector\n",
    "\n",
    "for k, v in list(train_labels_encoded.items())[:3]:\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f11ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.jpg: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "1.jpg: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2.jpg: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Validation dataset - Encoding\n",
    "\n",
    "import csv\n",
    "\n",
    "def normalize_label(label):\n",
    "    return label.strip().lower()\n",
    "\n",
    "class_name_to_id = {}\n",
    "with open(r\"C:\\Users\\Hithesh\\Downloads\\Kaush Stuff\\FoodSeg103 Stuff\\category_id.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            idx, name = parts\n",
    "            class_name_to_id[normalize_label(name)] = int(idx)\n",
    "\n",
    "num_classes = max(class_name_to_id.values()) + 1\n",
    "\n",
    "val_labels_encoded = {}\n",
    "with open(r\"C:\\Users\\Hithesh\\Downloads\\Kaush Stuff\\FoodSeg103_export\\validation\\labels.csv\", newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        filename = row['filename']\n",
    "        labels = row['labels'].split(',')\n",
    "        label_indices = []\n",
    "        for label in labels:\n",
    "            norm_label = normalize_label(label)\n",
    "            if norm_label in class_name_to_id:\n",
    "                idx = class_name_to_id[norm_label]\n",
    "                if idx < num_classes:\n",
    "                    label_indices.append(idx)\n",
    "        label_vector = [0] * num_classes\n",
    "        for idx in label_indices:\n",
    "            label_vector[idx] = 1\n",
    "        val_labels_encoded[filename] = label_vector\n",
    "\n",
    "for k, v in list(val_labels_encoded.items())[:3]:\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad78fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, labels, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.image_filenames = list(labels.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[img_name]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        return image, label\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_image_folder = r\"C:\\Users\\Hithesh\\Downloads\\Kaush Stuff\\FoodSeg103_export\\train\"\n",
    "val_image_folder = r\"C:\\Users\\Hithesh\\Downloads\\Kaush Stuff\\FoodSeg103_export\\validation\"\n",
    "\n",
    "train_dataset = CustomImageDataset(train_image_folder, train_labels_encoded, transform=train_transforms)\n",
    "val_dataset = CustomImageDataset(val_image_folder, val_labels_encoded, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 104\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    model = mobilenet_v3_small(pretrained=True)\n",
    "    model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "                inputs, labels = batch[0], batch[1]\n",
    "            else:\n",
    "                raise ValueError('Expected batch of (inputs, labels)')\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "                inputs, labels = batch[0], batch[1]\n",
    "            else:\n",
    "                raise ValueError('Expected batch of (inputs, labels)')\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += torch.numel(labels)\n",
    "\n",
    "    val_acc = correct / total\n",
    "    return val_acc\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(f\"Best validation accuracy: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dae444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.0719\n",
      "Epoch 1 validation loss: 0.0820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 training loss: 0.0687\n",
      "Epoch 2 validation loss: 0.0817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 training loss: 0.0683\n",
      "Epoch 3 validation loss: 0.0817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 training loss: 0.0668\n",
      "Epoch 4 validation loss: 0.0817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 training loss: 0.0658\n",
      "Epoch 5 validation loss: 0.0821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 training loss: 0.0651\n",
      "Epoch 6 validation loss: 0.0821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 training loss: 0.0648\n",
      "Epoch 7 validation loss: 0.0826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 training loss: 0.0640\n",
      "Epoch 8 validation loss: 0.0830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 training loss: 0.0635\n",
      "Epoch 9 validation loss: 0.0831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 training loss: 0.0622\n",
      "Epoch 10 validation loss: 0.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 training loss: 0.0622\n",
      "Epoch 11 validation loss: 0.0835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 training loss: 0.0619\n",
      "Epoch 12 validation loss: 0.0836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 training loss: 0.0614\n",
      "Epoch 13 validation loss: 0.0837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 training loss: 0.0609\n",
      "Epoch 14 validation loss: 0.0839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 training loss: 0.0601\n",
      "Epoch 15 validation loss: 0.0841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 training loss: 0.0594\n",
      "Epoch 16 validation loss: 0.0846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 training loss: 0.0597\n",
      "Epoch 17 validation loss: 0.0850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 training loss: 0.0589\n",
      "Epoch 18 validation loss: 0.0852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 training loss: 0.0586\n",
      "Epoch 19 validation loss: 0.0853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 training loss: 0.0577\n",
      "Epoch 20 validation loss: 0.0852\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_class_names(txt_path):\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "class_names = load_class_names(r\"C:\\Users\\Hithesh\\Downloads\\Kaush Stuff\\FoodSeg103 Stuff\\category_id.txt\")\n",
    "\n",
    "num_classes = len(class_names)\n",
    "model = mobilenet_v3_small(pretrained=False)\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "\n",
    "checkpoint_path = r\"C:\\Users\\Hithesh\\Downloads\\Kaush Stuff\\mobilenetv3_food_classifier.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in checkpoint.items():\n",
    "    new_key = k[len('mobilenet.'):] if k.startswith('mobilenet.') else k\n",
    "    new_state_dict[new_key] = v\n",
    "model.load_state_dict(new_state_dict, strict=False)  \n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_extra_epochs = 20\n",
    "\n",
    "for epoch in range(num_extra_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_extra_epochs}\", leave=False)\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        avg_loss = running_loss / (progress_bar.n if progress_bar.n > 0 else 1)\n",
    "        progress_bar.set_postfix(loss=avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} training loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1} validation loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"mobilenetv3_food_classifier_finetuned_more_epochs.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "\n",
    "torch.save(model.state_dict(), \"final_image_classifier.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
